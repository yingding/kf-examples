{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df4f1a2-5690-41c6-a061-274545e459a3",
   "metadata": {},
   "source": [
    "# About this Jupyter Notebook\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@updated: 30.09.2022\n",
    "\n",
    "This notebook demonstrate an example of kubeflow pipeline with python function, the kubeflow pipeline contains a single model training with tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cf343-6af6-4ba3-b48d-5985633a45e2",
   "metadata": {},
   "source": [
    "## Install KFP Python SDK to build a V1 pipeline\n",
    "* Build KF pipeline with python SDK: https://www.kubeflow.org/docs/components/pipelines/sdk/build-pipeline/\n",
    "* Current KFP python SDK version on pypi.org: https://pypi.org/project/kfp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e4c76-43fd-460d-9457-20687bc552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394d5d41-1b7a-44ca-abfd-b787756c2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp-kubernetes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip uninstall -y kfp-kubernetes==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e083866-51d7-400f-9cfd-1790675a6b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==1.8.22 in /home/jovyan/.local/lib/python3.11/site-packages (1.8.22)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (1.4.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (6.0.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (2.12.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=1.20.0 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (2.13.0)\n",
      "Requirement already satisfied: kubernetes<26,>=8.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (25.3.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (1.12.11)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (2.23.4)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (0.10.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (2.2.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (1.8.5)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (4.19.2)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (8.1.7)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (1.2.14)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (0.1.10)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (0.15)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.16 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (0.1.16)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (0.5.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (3.20.3)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.11/site-packages (from kfp==1.8.22) (1.26.18)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (1.10.13)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/jovyan/.local/lib/python3.11/site-packages (from kfp==1.8.22) (0.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/jovyan/.local/lib/python3.11/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.22) (1.16.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fire<1,>=0.3.1->kfp==1.8.22) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/jovyan/.local/lib/python3.11/site-packages (from fire<1,>=0.3.1->kfp==1.8.22) (2.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (1.61.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (2.31.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/jovyan/.local/lib/python3.11/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.22) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/jovyan/.local/lib/python3.11/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.22) (0.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==1.8.22) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=1.20.0->kfp==1.8.22) (1.5.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5,>=3.0.1->kfp==1.8.22) (0.10.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.22) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.22) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (68.2.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (1.6.4)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<26,>=8.0.0->kfp==1.8.22) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<2,>=1.8.2->kfp==1.8.22) (4.8.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.22) (0.41.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.11/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.22) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==1.8.22) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.22) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp==1.8.22) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade --user kfp==1.8.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbec4-7f31-4cbf-bc8a-b852271b4884",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "After the installation of KFP python SDK, the notebook kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3811-f3c1-45f2-8476-719a937caadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting familiar with Jupyter Notebook ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5c31cc-0869-48eb-9587-89df6aff7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current platform python version: 3.11.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print (f\"current platform python version: {python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c90732-087d-46de-9a92-c6eb85e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                                                kf-resource-quota\n",
      "Namespace:                                                           kubeflow-kindfor\n",
      "Resource                                                             Used     Hard\n",
      "--------                                                             ----     ----\n",
      "cpu                                                                  2810m    36\n",
      "csi-s3.storageclass.storage.k8s.io/persistentvolumeclaims            0        10\n",
      "csi-s3.storageclass.storage.k8s.io/requests.storage                  0        2Ti\n",
      "kubeflow-nfs-csi.storageclass.storage.k8s.io/persistentvolumeclaims  6        20\n",
      "kubeflow-nfs-csi.storageclass.storage.k8s.io/requests.storage        47Gi     4Ti\n",
      "memory                                                               11406Mi  520Gi\n",
      "minio-nfs-csi.storageclass.storage.k8s.io/persistentvolumeclaims     2        20\n",
      "minio-nfs-csi.storageclass.storage.k8s.io/requests.storage           210Gi    10Ti\n"
     ]
    }
   ],
   "source": [
    "# run kubectl command line to see the quota in the name space\n",
    "!kubectl describe quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c70007-489a-49d2-baff-eaeaa7a984a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                           1.8.22\n",
      "kfp-pipeline-spec             0.1.16\n",
      "kfp-server-api                1.8.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.5.1\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986797-6418-4f13-a439-7dfe2e6c09a3",
   "metadata": {},
   "source": [
    "## Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bcd262-cbbb-4914-90f0-9c6dc85d5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-kindfor\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "EXPERIMENT_NAME = 'demo' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'this kf experiments loads iris data from tf dataset and build models'\n",
    "PREFIX = \"single_\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f8e8f5-e74d-4128-b6d1-5d0b371a455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(tf_datasets='4.9.2', pandas_version='1.5.3', jinja2_version='3.1.2', sklearn_version='1.2.2', numpy_version='1.24.2', base_tf_image='tensorflow/tensorflow:2.12.0', base_python_image='python:3.10.11')\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Settings = namedtuple('Settings', [\n",
    "    # 'tf_io', \n",
    "    'tf_datasets',\n",
    "    'pandas_version',\n",
    "    'jinja2_version',\n",
    "    'sklearn_version',\n",
    "    'numpy_version',\n",
    "    'base_tf_image',\n",
    "    'base_python_image'\n",
    "])\n",
    "# the base images are from the dockerhub https://hub.docker.com/_/python\n",
    "# settings = Settings(\n",
    "#     tf_io=\"0.27.0\", \n",
    "#     tf_datasets=\"4.6.0\",\n",
    "#     pandas_version=\"1.5.0\",\n",
    "#     jinja2_version=\"3.1.2\",\n",
    "#     sklearn_version=\"1.1.2\", # scikit-learn\n",
    "#     numpy_version=\"1.23.3\",\n",
    "#     base_tf_image=\"tensorflow/tensorflow:2.10.0\",\n",
    "#     base_python_image=\"python:3.8.14\"\n",
    "# )\n",
    "\n",
    "settings = Settings(\n",
    "    # tf_io=\"0.27.0\", \n",
    "    tf_datasets=\"4.9.2\",\n",
    "    pandas_version=\"1.5.3\",\n",
    "    jinja2_version=\"3.1.2\",\n",
    "    sklearn_version=\"1.2.2\", # scikit-learn\n",
    "    numpy_version=\"1.24.2\",\n",
    "    base_tf_image=\"tensorflow/tensorflow:2.12.0\",\n",
    "    base_python_image=\"python:3.10.11\"\n",
    ") \n",
    "print(f\"{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18f24-d264-4352-aaf0-e1ee8adaadfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating KubeFlow component from python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3226c3-b0d9-4c2b-9f48-2927f6433aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kfp dsl components\n",
    "import kfp.dsl as dsl\n",
    "from functools import partial\n",
    "from kfp.dsl import (\n",
    "    pipeline,\n",
    "    ContainerOp\n",
    ")\n",
    "from kfp.components import (\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    create_component_from_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427c5f-3bdb-40d4-8d44-3fef37d4e1c1",
   "metadata": {},
   "source": [
    "#### Create download component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf024d03-f8aa-486e-8157-1c3c3cc92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}demo_download_component.yaml\",\n",
    "    base_image=settings.base_tf_image, # use tf base image\n",
    "    packages_to_install=[\n",
    "        f\"tensorflow-datasets=={settings.tf_datasets}\",\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"Jinja2=={settings.jinja2_version}\", # needed by tf dataset\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def download_data(output_path: OutputPath(\"CSV\")):\n",
    "    # https://www.tensorflow.org/datasets/keras_example\n",
    "    # something about iris dataset\n",
    "    # https://www.tensorflow.org/datasets/catalog/iris\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    (ds_train), ds_info = tfds.load(\n",
    "        'iris',\n",
    "        split=tfds.Split.TRAIN,\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True)\n",
    "    # assert type\n",
    "    assert isinstance(ds_train, tf.data.Dataset)\n",
    "    size = ds_train.cardinality().numpy()\n",
    "    \n",
    "    # convert to pandas dataframe\n",
    "    df = tfds.as_dataframe(ds_train.take(size), ds_info)\n",
    "    \n",
    "    # export csv data without index\n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df.to_csv(f, index=False, header=True, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f1ed-4564-4302-8fb9-094e1325ebe0",
   "metadata": {},
   "source": [
    "#### Create data processing component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75394f2f-edd3-430f-bf0b-5e897faf5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}process_iris_data_component.yaml\",\n",
    "    base_image=settings.base_python_image, # use python base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def process_data(label_col_name: str, feature_col_name: str, input_path: InputPath(\"CSV\"), train_output_path: OutputPath(\"CSV\"), test_output_path: OutputPath(\"CSV\")):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import DataFrame\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df = pd.read_csv(input_path, sep=\",\", header=0, index_col=None)\n",
    "    \n",
    "    print(\"input csv dataframe\")\n",
    "    print(df)\n",
    "    print(df.shape)\n",
    "    \n",
    "    def iris_ndarray_to_feature_columns(df: DataFrame, feature_col_name=\"features\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        not inplace function, return a transformed DataFrame\n",
    "        \"\"\"\n",
    "        # testing the DataFrame having two column\n",
    "        assert df.shape[1]==2\n",
    "        feature_names = {\n",
    "            \"0\": \"sepal length (cm)\",\n",
    "            \"1\": \"sepal width (cm)\",\n",
    "            \"2\": \"petal length (cm)\",\n",
    "            \"3\": \"petal width (cm)\",\n",
    "        }\n",
    "        # convert column of string to column of numpy array\n",
    "        df['numpy'] = df[feature_col_name].apply(lambda x: \n",
    "                           np.fromstring(\n",
    "                               x.replace('[','')\n",
    "                                .replace(']',''), sep=' '))\n",
    "        # convert numpy array element to feature column with name\n",
    "        for i in range(0, 4):\n",
    "            df[feature_names[f\"{i}\"]] = df[\"numpy\"].apply(lambda x: x[i])\n",
    "        # remove\n",
    "        return df.drop(columns=[feature_col_name, 'numpy'])\n",
    "    \n",
    "    # unpack the numpy array feature to feature columns\n",
    "    df = iris_ndarray_to_feature_columns(df, feature_col_name)\n",
    "    \n",
    "    # split\n",
    "    all_feature_cols_mask = ~df.columns.isin([label_col_name])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.loc[:, all_feature_cols_mask], df.loc[:, [label_col_name]], test_size=0.2, random_state=0)\n",
    "    \n",
    "    # join on index\n",
    "    df_train = X_train.join(y_train) \n",
    "    df_test = X_test.join(y_test)\n",
    "    print(f\"df_train.shape {df_train.shape}\")\n",
    "    print(f\"df_test.shape {df_test.shape}\")\n",
    "    \n",
    "    # get row by index label\n",
    "    # print(df_train.loc[137])\n",
    "    \n",
    "    # output training set\n",
    "    with open(train_output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df_train.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    \n",
    "    # output test set\n",
    "    with open(test_output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df_test.to_csv(f, index=False, header=True, encoding=\"utf-8\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efd8d2-96c0-413f-bbbe-c5471cf205ec",
   "metadata": {},
   "source": [
    "#### Create TensorFlow model\n",
    "\n",
    "* Example of creating TensorFlow FNN model with iris dataset: https://medium.com/@nutanbhogendrasharma/tensorflow-deep-learning-model-with-iris-dataset-8ec344c49f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274b10e9-1b44-4ee4-a64b-87c9e3dc1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}train_tf_iris_data_component.yaml\",\n",
    "    base_image=settings.base_tf_image, # use tensorflow base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def train_tf(\n",
    "    batch_size: int, epochs: int, label_col_name: str, train_input_path: InputPath(\"CSV\"), test_input_path: InputPath(\"CSV\"),\n",
    "    cm_output_path: OutputPath(\"CSV\")\n",
    ") -> NamedTuple('ModelScores', [('model', str), ('acc', float), ('f1', float)]): \n",
    "    import json\n",
    "    from pandas import DataFrame\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    from collections import namedtuple\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"tf.__version {tf.__version__}\")\n",
    "    \n",
    "    df_train = pd.read_csv(train_input_path, sep=\",\", header=0, index_col=None)\n",
    "    df_test = pd.read_csv(test_input_path, sep=\",\", header=0, index_col=None)\n",
    "    \n",
    "    def get_feature_label(df: DataFrame, label_col_name: str):\n",
    "        \"\"\"\n",
    "        local util function to split feature and label dataframe\n",
    "        \"\"\"\n",
    "        all_feature_cols_mask = ~df_train.columns.isin([label_col_name])\n",
    "        x_train_l = df.loc[:, all_feature_cols_mask]\n",
    "        y_train_l = pd.get_dummies(df[label_col_name])\n",
    "        # y_train_l = pd.get_dummies(df[label_col_name]).values\n",
    "        return x_train_l, y_train_l\n",
    "        \n",
    "    X_train, y_train = get_feature_label(df_train, label_col_name)\n",
    "    print(\"train df\")\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    \n",
    "    print(\"train values\")\n",
    "    print(X_train.to_numpy())\n",
    "    print(y_train.to_numpy())\n",
    "    \n",
    "    print(f\"training with batch size: {batch_size}, epoch: {epochs}\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    # config\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # model.compile(\n",
    "    #     optimizer=tf.optimizers.RMSprop(),\n",
    "    #     loss='categorical_crossentropy',\n",
    "    #     metrics=['accuracy'])\n",
    "    \n",
    "    # train model\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy(), batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    # show the shape and param of the ANN model\n",
    "    # note: summary() must be called after fit() or build()\n",
    "    model.summary()\n",
    "    \n",
    "    # evaluate model\n",
    "    print(\"evaluate model\")\n",
    "    X_test, y_test = get_feature_label(df_test, label_col_name)\n",
    "    loss, accuracy = model.evaluate(X_test.to_numpy(), y_test.to_numpy(), verbose=0)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "      \n",
    "    # predict model \n",
    "    y_pred: np.ndarray = model.predict(X_test.to_numpy())\n",
    "    \n",
    "    # convert the numpy triple probability to prediction of iris category\n",
    "    actual = np.argmax(y_test.to_numpy(), axis=1)\n",
    "    predicted = np.argmax(y_pred, axis=1)\n",
    "    print(f\"actual: {actual}\")\n",
    "    print(f\"predicted: {predicted}\")\n",
    "    \n",
    "    # for multi-class cls with weighted f1\n",
    "    AVERAGE_MODE = \"weighted\"\n",
    "    f1 = f1_score(actual, predicted, average=AVERAGE_MODE) \n",
    "    print(f\"Test f1: {f1}\")\n",
    "    \n",
    "    def create_iris_confusion_matrix_helper(actual, predicted) -> DataFrame:\n",
    "        \"\"\" create a confustion matrix DataFrame\n",
    "        @param: actual: array-like of shape (n_samples,)\n",
    "        @param: predicted: array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        label_map = {\n",
    "            \"0\": \"Iris-setosa\",\n",
    "            \"1\": \"Iris-versicolor\",\n",
    "            \"2\": \"Iris-virginica\"\n",
    "        }\n",
    "        # the keys are str, need to be converted to int as labels\n",
    "        # for the funciton confusion_matrix\n",
    "        labels = list(map(int, label_map.keys()))\n",
    "        vocab = list(label_map.values())\n",
    "        # vocab = list(np.unique(actual))\n",
    "        cm: np.ndarray = confusion_matrix(actual, predicted, labels = labels)\n",
    "        return pd.DataFrame(cm, index=vocab, columns=vocab)    \n",
    "    \n",
    "    conf_mat_df = create_iris_confusion_matrix_helper(actual, predicted)\n",
    "    print(conf_mat_df)\n",
    "    \n",
    "    # write conf_mat_df to the output path\n",
    "    with open(cm_output_path, 'w', encoding=\"utf-8\") as f:\n",
    "        conf_mat_df.to_csv(f, index=True, encoding=\"utf-8\")\n",
    "        \n",
    "    mod_scores = namedtuple('ModelScores',['model', 'acc', 'f1'])    \n",
    "    return mod_scores(\"tf\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f7c6f-39c3-4a75-8d54-0547288ebb30",
   "metadata": {},
   "source": [
    "### Define Confusion Matrix Visualization Component\n",
    "* Visualization with Kubeflow: https://www.kubeflow.org/docs/components/pipelines/v1/sdk/output-viewer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25baa12f-12f6-4a53-867d-2d6fb1826661",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    output_component_file=f\"{PREFIX}visualize_confusion_matrix_component.yaml\",\n",
    "    base_image=settings.base_python_image, # use python base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "    ]    \n",
    ")\n",
    "def confusion_visualization(csv_path: InputPath(), mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "    \"\"\"Provide confusion matrix csv file to visualize as metrics.\"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from typing import List, Tuple\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    cm_df = pd.read_csv(csv_path, index_col=0)\n",
    "    print(cm_df)\n",
    "    \n",
    "    def make_pair_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, List]: \n",
    "        \"\"\"\n",
    "        this function constructs a target, predicted, count pair dataframe,\n",
    "        to be used for kf v1 confusion matrix visualiation\n",
    "        \"\"\"\n",
    "        # create a vocabular list from the dataframe column names as List[str]\n",
    "        vocab = list(map(str, df.columns.values.tolist()))\n",
    "        data = []\n",
    "        # use df.to_numpy() to remove the header row for enumerate\n",
    "        for target_index, target_row in enumerate(cm_df.to_numpy()):\n",
    "            for predicted_index, count in enumerate(target_row):\n",
    "                data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "            \n",
    "        # convert confusion_matrix pair dataset to dataframe\n",
    "        df = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "\n",
    "        # change 'target', 'predicted' to strings\n",
    "        # df[['target', 'predicted']] = (df[['target', 'predicted']].astype(\"string\")).astype(\"string\")\n",
    "\n",
    "        vocab = cm_df.columns.values.tolist()\n",
    "        return df, vocab\n",
    "    \n",
    "    pair_df, vocab = make_pair_dataframe(cm_df)\n",
    "    # print(pair_df)\n",
    "    # print(pair_df.dtypes)\n",
    "    # print(f\"type of vocab: {type(vocab)}\")\n",
    "    # print(vocab)\n",
    "    \n",
    "    '''\n",
    "    Important: \n",
    "        make sure the 'label' is a List[str]\n",
    "        use list(map(str, vocab)) to convert, if not\n",
    "    '''\n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'confusion_matrix',\n",
    "          'format': 'csv',\n",
    "          'schema': [\n",
    "            {'name': 'target', 'type': 'CATEGORY'},\n",
    "            {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "            {'name': 'count', 'type': 'NUMBER'},\n",
    "          ],\n",
    "          'source': pair_df.to_csv(header=False, index=False),\n",
    "          'storage': 'inline',\n",
    "          'labels': vocab,\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9b0bb-0938-4371-add7-046a3c9018bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Run Output Component\n",
    "Reference to KF v1 Pipeline Metrics: \n",
    "* https://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-metrics/\n",
    "\n",
    "The `name` and `numberValue` variable shall not be changed.\n",
    "```json\n",
    "            {\n",
    "                \"name\": \"some name\",\n",
    "                \"numberValue\": some value,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"f1 score\",\n",
    "                \"numberValue\": metric_tf,\n",
    "            },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e46208-1558-4eca-9070-b2310571b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "@create_component_from_func\n",
    "def show_performance_metrics(\n",
    "    model_code: str,\n",
    "    metric_f1: float,\n",
    "    metric_acc: float,\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "):\n",
    "    import json\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"name\": f\"{model_code}-acc-score\", \n",
    "                \"numberValue\": metric_acc,\n",
    "                \"format\": \"PERCENTAGE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": f\"{model_code}-f1-score\",\n",
    "                \"numberValue\": metric_f1,\n",
    "                \"format\": \"PERCENTAGE\"\n",
    "            },\n",
    "        ],\n",
    "    } \n",
    "    with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165590ad-f3ba-482f-bcdc-16635f5ab69c",
   "metadata": {},
   "source": [
    "### Define Helper Function\n",
    "Difference between 2Gi and 2G\\\n",
    "https://stackoverflow.com/questions/50804915/kubernetes-size-definitions-whats-the-difference-of-gi-and-g/50805048#50805048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb43819c-aeb2-4ab0-a395-54dc3c3bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_resource_transformer(op: ContainerOp, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m'):\n",
    "    \"\"\"\n",
    "    this function helps to set the resource limit for container operators\n",
    "    op.set_memory_limit('1000Mi') = 1GB\n",
    "    op.set_cpu_limit('1000m') = 1 cpu core\n",
    "    \"\"\"\n",
    "    return op.set_memory_request(mem_req)\\\n",
    "            .set_memory_limit(mem_lim)\\\n",
    "            .set_cpu_request(cpu_req)\\\n",
    "            .set_cpu_limit(cpu_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30253e28-bf1e-4716-a05b-2524952da9b5",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "* Intro Kubeflow pipeline: https://v1-5-branch.kubeflow.org/docs/components/pipelines/introduction/\n",
    "* Kubeflow pipeline SDK v1: https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f43545d-ddfd-49ad-8f43-fb7600d4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def custom_pipeline(epochs: int):\n",
    "    '''local variable'''\n",
    "    no_artifact_cache = \"P0D\"\n",
    "    artifact_cache_today = \"P1D\"\n",
    "    cache_setting = no_artifact_cache\n",
    "    label_col_name = \"label\"\n",
    "    feature_col_name = \"features\"\n",
    "    batch_size = 50\n",
    "    # epochs = 100\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    download_task = download_data()\n",
    "    # 200 MB ram and 1 cpu\n",
    "    download_task = pod_resource_transformer(download_task, mem_req=\"500Mi\", cpu_req=\"1000m\")\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    # download_task.execution_options.caching_strategy.max_cache_staleness = artifact_cache_today\n",
    "    download_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    download_task.set_display_name(\"download iris data\")\n",
    "    \n",
    "    # variable name \"output_path\", all \"_path\" will be removed by sysem\n",
    "    process_data_task = process_data(label_col_name, feature_col_name, download_task.outputs[\"output\"])\n",
    "    process_data_task = pod_resource_transformer(process_data_task, mem_req=\"500Mi\", cpu_req=\"1000m\")\n",
    "    process_data_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    process_data_task.set_display_name(\"split iris data\")\n",
    "    \n",
    "    # train tensorflow model, input variable are all removed with _path  \n",
    "    train_tf_task = train_tf(\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        label_col_name=label_col_name,\n",
    "        train_input=process_data_task.outputs[\"train_output\"],\n",
    "        test_input=process_data_task.outputs[\"test_output\"]\n",
    "    )\n",
    "    train_tf_task = pod_resource_transformer(train_tf_task, mem_req=\"2Gi\", cpu_req=\"2000m\")\n",
    "    train_tf_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    train_tf_task.set_display_name(\"train tf model\")\n",
    "    \n",
    "    # visualize confusion matrix\n",
    "    visualization_task = confusion_visualization(train_tf_task.outputs[\"cm_output\"])\n",
    "    visualization_task = pod_resource_transformer(visualization_task, mem_req=\"200Mi\", cpu_req=\"500m\")\n",
    "    visualization_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    visualization_task.set_display_name(\"visualize confusion matrix\")\n",
    "    \n",
    "    # show model outputs\n",
    "    show_ml_metrics_task = show_performance_metrics(\n",
    "        model_code = train_tf_task.outputs['model'],\n",
    "        metric_f1 = train_tf_task.outputs['f1'],\n",
    "        metric_acc =  train_tf_task.outputs['acc'],\n",
    "    )\n",
    "    show_ml_metrics_task = pod_resource_transformer(show_ml_metrics_task, mem_req=\"200Mi\", cpu_req=\"500m\")\n",
    "    show_ml_metrics_task.execution_options.caching_strategy.max_cache_staleness =cache_setting\n",
    "    show_ml_metrics_task.set_display_name(\"output model metrics\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e789aa-5ccc-4290-8d9d-b3297128417b",
   "metadata": {},
   "source": [
    "### (optional) pipeline compile step\n",
    "use the following command to compile the pipeline to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb19bcb-2215-4ca6-93a8-bd65a31f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPE_LINE_FILE_NAME=f\"{PREFIX}kfp_iris_demo_pipeline\"\n",
    "kfp.compiler.Compiler().compile(custom_pipeline, f\"{PIPE_LINE_FILE_NAME}.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198c16-f9ab-4e4c-a2f2-d710bec7aba9",
   "metadata": {},
   "source": [
    "### Create Experiment Run\n",
    "\n",
    "create run label with current data time\n",
    "```python\n",
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "ts = datetime.strftime(datetime.now(ptimezone(\"Europe/Berlin\")), \"%Y-%m-%d %H-%M-%S\")\n",
    "print(ts)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/25837452/python-get-current-time-in-right-timezone/25887393#25887393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08dd9330-1a3f-4fcd-8e53-fa7e4a05de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1ed73-9fd8-4e59-855e-690e738d09d9",
   "metadata": {},
   "source": [
    "### Config pipeline run\n",
    "* Setting imagePullSecretes for Pipeline with SDK: https://github.com/kubeflow/pipelines/issues/5843#issuecomment-859799181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853837b3-604d-439b-9cea-4b8e226c0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    \"epochs\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5264dc44-9abd-48a2-970e-3ed8693edfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/557b0023-3f89-4df6-bf8e-5091a646aa89\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/62c7ccd4-4548-49f3-ac20-e89063508686\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=62c7ccd4-4548-49f3-ac20-e89063508686)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME = f\"{PREFIX}kfp_iris_demo {get_local_time_str()}\"\n",
    "\n",
    "# client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline_func=custom_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6d15-c88d-4d5a-9b4e-fb2f3706c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
