{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df4f1a2-5690-41c6-a061-274545e459a3",
   "metadata": {},
   "source": [
    "# About this Jupyter Notebook\n",
    "\n",
    "@author: Yingding Wang\\\n",
    "@created: 30.06.2022\\\n",
    "@updated: 05.01.2024\\\n",
    "@version: 2\n",
    "\n",
    "This notebook demonstrates an example of kubeflow pipeline with python function, the kubeflow pipeline contains a single model training with tensorflow using the KFP Python SDK v2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cf343-6af6-4ba3-b48d-5985633a45e2",
   "metadata": {},
   "source": [
    "## Install KFP Python SDK to build a V2 pipeline\n",
    "* Build KF pipeline with python SDK: https://www.kubeflow.org/docs/components/pipelines/sdk/build-pipeline/\n",
    "* Current KFP python SDK version on pypi.org: https://pypi.org/project/kfp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e4c76-43fd-460d-9457-20687bc552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b9a3d2-d6e7-4ffe-9481-1f560bf1def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install --upgrade --user kfp[kubernetes]==2.6.0\n",
    "# installs the following packages\n",
    "# !{sys.executable} -m pip install --upgrade --user kfp==2.6.0 kfp-kubernetes==1.1.0 kfp-pipeline-spec==0.3.0 kfp-server-api==2.0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbec4-7f31-4cbf-bc8a-b852271b4884",
   "metadata": {},
   "source": [
    "## Restart the Kernel\n",
    "\n",
    "After the installation of KFP python SDK, the notebook kernel must be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f3811-f3c1-45f2-8476-719a937caadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting familiar with Jupyter Notebook ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c70007-489a-49d2-baff-eaeaa7a984a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfp                           2.6.0\n",
      "kfp-kubernetes                1.1.0\n",
      "kfp-pipeline-spec             0.3.0\n",
      "kfp-server-api                2.0.5\n"
     ]
    }
   ],
   "source": [
    "# examing the kfp python sdk version inside a KubeFlow v1.8.0\n",
    "!{sys.executable} -m pip list | grep kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5c31cc-0869-48eb-9587-89df6aff7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current platform python version: 3.11.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print (f\"current platform python version: {python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c90732-087d-46de-9a92-c6eb85e8a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                                                                kf-resource-quota\n",
      "Namespace:                                                           kubeflow-kindfor\n",
      "Resource                                                             Used     Hard\n",
      "--------                                                             ----     ----\n",
      "cpu                                                                  2585m    36\n",
      "csi-s3.storageclass.storage.k8s.io/persistentvolumeclaims            0        10\n",
      "csi-s3.storageclass.storage.k8s.io/requests.storage                  0        2Ti\n",
      "kubeflow-nfs-csi.storageclass.storage.k8s.io/persistentvolumeclaims  4        20\n",
      "kubeflow-nfs-csi.storageclass.storage.k8s.io/requests.storage        45Gi     4Ti\n",
      "memory                                                               11150Mi  520Gi\n",
      "minio-nfs-csi.storageclass.storage.k8s.io/persistentvolumeclaims     2        20\n",
      "minio-nfs-csi.storageclass.storage.k8s.io/requests.storage           210Gi    10Ti\n"
     ]
    }
   ],
   "source": [
    "# run kubectl command line to see the quota in the name space\n",
    "!kubectl describe quota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c986797-6418-4f13-a439-7dfe2e6c09a3",
   "metadata": {},
   "source": [
    "## Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bcd262-cbbb-4914-90f0-9c6dc85d5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubeflow-kindfor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "client = kfp.Client()\n",
    "NAMESPACE = client.get_user_namespace()\n",
    "EXPERIMENT_NAME = 'demo' # Name of the experiment in the KF webapp UI\n",
    "EXPERIMENT_DESC = 'this kf experiments loads iris data from tf dataset and build models'\n",
    "PREFIX = \"single_\"\n",
    "\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f8e8f5-e74d-4128-b6d1-5d0b371a455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings(tf_datasets='4.9.4', pandas_version='2.1.4', jinja2_version='3.1.2', sklearn_version='1.3.2', numpy_version='1.26.3', base_tf_image='tensorflow/tensorflow:2.14.0', base_python_image='python:3.11.7-bullseye')\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Settings:\n",
    "    tf_datasets: str = \"4.9.4\" # \"4.9.2\"\n",
    "    pandas_version: str = \"2.1.4\" #\"1.5.3\"\n",
    "    jinja2_version: str = \"3.1.2\"\n",
    "    sklearn_version: str = \"1.3.2\" # \"1.2.2\"\n",
    "    numpy_version: str = \"1.26.3\" #\"1.24.2\",\n",
    "    base_tf_image: str = \"tensorflow/tensorflow:2.14.0\"\n",
    "    # base_tf_image: str = \"tensorflow/tensorflow:2.12.0\"\n",
    "    # base_python_image: str = \"python:3.10.11\"\n",
    "    base_python_image: str = \"python:3.11.7-bullseye\"\n",
    "    \n",
    "\n",
    "settings = Settings()\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18f24-d264-4352-aaf0-e1ee8adaadfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating KubeFlow component from python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3226c3-b0d9-4c2b-9f48-2927f6433aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "from kfp.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83427c5f-3bdb-40d4-8d44-3fef37d4e1c1",
   "metadata": {},
   "source": [
    "#### Create download component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf024d03-f8aa-486e-8157-1c3c3cc92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=settings.base_tf_image, # use tf base image\n",
    "    packages_to_install=[\n",
    "        f\"tensorflow-datasets=={settings.tf_datasets}\",\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"Jinja2=={settings.jinja2_version}\", # needed by tf dataset\n",
    "        f\"urllib3==1.26.18\",\n",
    "        f\"kfp==2.5.0\"\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def download_data(csv_dataset: Output[Dataset]):\n",
    "    # https://www.tensorflow.org/datasets/keras_example\n",
    "    # something about iris dataset\n",
    "    # https://www.tensorflow.org/datasets/catalog/iris\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    (ds_train), ds_info = tfds.load(\n",
    "        'iris',\n",
    "        split=tfds.Split.TRAIN,\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True)\n",
    "    # assert type\n",
    "    assert isinstance(ds_train, tf.data.Dataset)\n",
    "    size = ds_train.cardinality().numpy()\n",
    "    \n",
    "    # convert to pandas dataframe\n",
    "    df = tfds.as_dataframe(ds_train.take(size), ds_info)\n",
    "    \n",
    "    # export csv data without index\n",
    "    # with open(output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    #    df.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    with open(csv_dataset.path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    csv_dataset.metadata['type'] = 'csv'    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f1ed-4564-4302-8fb9-094e1325ebe0",
   "metadata": {},
   "source": [
    "#### Create data processing component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75394f2f-edd3-430f-bf0b-5e897faf5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=settings.base_python_image, # use python base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def process_data(label_col_name: str, feature_col_name: str, \n",
    "                 csv_dataset: Input[Dataset], \n",
    "                 train_csv_dataset: Output[Dataset], \n",
    "                 test_csv_dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pandas import DataFrame\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    with open(csv_dataset.path, \"r\", encoding=\"utf-8\") as f:\n",
    "        df = pd.read_csv(f, sep=\",\", header=0, index_col=None)\n",
    "    \n",
    "    print(\"input csv dataframe\")\n",
    "    print(df)\n",
    "    print(df.shape)\n",
    "    \n",
    "    def iris_ndarray_to_feature_columns(df: DataFrame, feature_col_name=\"features\") -> DataFrame:\n",
    "        \"\"\"\n",
    "        not inplace function, return a transformed DataFrame\n",
    "        \"\"\"\n",
    "        # testing the DataFrame having two column\n",
    "        assert df.shape[1]==2\n",
    "        feature_names = {\n",
    "            \"0\": \"sepal length (cm)\",\n",
    "            \"1\": \"sepal width (cm)\",\n",
    "            \"2\": \"petal length (cm)\",\n",
    "            \"3\": \"petal width (cm)\",\n",
    "        }\n",
    "        # convert column of string to column of numpy array\n",
    "        df['numpy'] = df[feature_col_name].apply(lambda x: \n",
    "                           np.fromstring(\n",
    "                               x.replace('[','')\n",
    "                                .replace(']',''), sep=' '))\n",
    "        # convert numpy array element to feature column with name\n",
    "        for i in range(0, 4):\n",
    "            df[feature_names[f\"{i}\"]] = df[\"numpy\"].apply(lambda x: x[i])\n",
    "        # remove\n",
    "        return df.drop(columns=[feature_col_name, 'numpy'])\n",
    "    \n",
    "    # unpack the numpy array feature to feature columns\n",
    "    df = iris_ndarray_to_feature_columns(df, feature_col_name)\n",
    "    \n",
    "    # split\n",
    "    all_feature_cols_mask = ~df.columns.isin([label_col_name])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.loc[:, all_feature_cols_mask], df.loc[:, [label_col_name]], test_size=0.2, random_state=0)\n",
    "    \n",
    "    # join on index\n",
    "    df_train = X_train.join(y_train) \n",
    "    df_test = X_test.join(y_test)\n",
    "    print(f\"df_train.shape {df_train.shape}\")\n",
    "    print(f\"df_test.shape {df_test.shape}\")\n",
    "    \n",
    "    # get row by index label\n",
    "    # print(df_train.loc[137])\n",
    "    \n",
    "    # output training set\n",
    "    # with open(train_output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    #     df_train.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    with open(train_csv_dataset.path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df_train.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    \n",
    "    # output test set\n",
    "    # with open(test_output_path, \"w+\", encoding=\"utf-8\") as f:\n",
    "    #     df_test.to_csv(f, index=False, header=True, encoding=\"utf-8\")\n",
    "    with open(test_csv_dataset.path, \"w+\", encoding=\"utf-8\") as f:\n",
    "        df_test.to_csv(f, index=False, header=True, encoding=\"utf-8\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efd8d2-96c0-413f-bbbe-c5471cf205ec",
   "metadata": {},
   "source": [
    "#### Create TensorFlow model\n",
    "\n",
    "* Example of creating TensorFlow FNN model with iris dataset: https://medium.com/@nutanbhogendrasharma/tensorflow-deep-learning-model-with-iris-dataset-8ec344c49f91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "274b10e9-1b44-4ee4-a64b-87c9e3dc1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "@dsl.component(\n",
    "    base_image=settings.base_tf_image, # use tensorflow base image\n",
    "    packages_to_install=[\n",
    "        f\"pandas=={settings.pandas_version}\",\n",
    "        f\"scikit-learn=={settings.sklearn_version}\",\n",
    "        f\"numpy=={settings.numpy_version}\",\n",
    "    ] # adding additional libs\n",
    ")\n",
    "def train_tf(\n",
    "    batch_size: int, epochs: int, label_col_name: str, \n",
    "    train_csv_dataset: Input[Dataset], test_csv_dataset: Input[Dataset],\n",
    "    cm_output_dataset: Output[Dataset]\n",
    ") -> NamedTuple('ModelScores', [('model', str), ('acc', float), ('f1', float)]): \n",
    "    import json\n",
    "    from pandas import DataFrame\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import confusion_matrix, f1_score\n",
    "    from collections import namedtuple\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"tf.__version {tf.__version__}\")\n",
    "    \n",
    "    df_train = pd.read_csv(train_csv_dataset.path, sep=\",\", header=0, index_col=None)\n",
    "    df_test = pd.read_csv(test_csv_dataset.path, sep=\",\", header=0, index_col=None)\n",
    "    \n",
    "    def get_feature_label(df: DataFrame, label_col_name: str):\n",
    "        \"\"\"\n",
    "        local util function to split feature and label dataframe\n",
    "        \"\"\"\n",
    "        all_feature_cols_mask = ~df_train.columns.isin([label_col_name])\n",
    "        x_train_l = df.loc[:, all_feature_cols_mask]\n",
    "        y_train_l = pd.get_dummies(df[label_col_name])\n",
    "        # y_train_l = pd.get_dummies(df[label_col_name]).values\n",
    "        return x_train_l, y_train_l\n",
    "        \n",
    "    X_train, y_train = get_feature_label(df_train, label_col_name)\n",
    "    print(\"train df\")\n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    \n",
    "    print(\"train values\")\n",
    "    print(X_train.to_numpy())\n",
    "    print(y_train.to_numpy())\n",
    "    \n",
    "    print(f\"training with batch size: {batch_size}, epoch: {epochs}\")\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    # config\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    # model.compile(\n",
    "    #     optimizer=tf.optimizers.RMSprop(),\n",
    "    #     loss='categorical_crossentropy',\n",
    "    #     metrics=['accuracy'])\n",
    "    \n",
    "    # train model\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy(), batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    # show the shape and param of the ANN model\n",
    "    # note: summary() must be called after fit() or build()\n",
    "    model.summary()\n",
    "    \n",
    "    # evaluate model\n",
    "    print(\"evaluate model\")\n",
    "    X_test, y_test = get_feature_label(df_test, label_col_name)\n",
    "    loss, accuracy = model.evaluate(X_test.to_numpy(), y_test.to_numpy(), verbose=0)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "      \n",
    "    # predict model \n",
    "    y_pred: np.ndarray = model.predict(X_test.to_numpy())\n",
    "    \n",
    "    # convert the numpy triple probability to prediction of iris category\n",
    "    actual = np.argmax(y_test.to_numpy(), axis=1)\n",
    "    predicted = np.argmax(y_pred, axis=1)\n",
    "    print(f\"actual: {actual}\")\n",
    "    print(f\"predicted: {predicted}\")\n",
    "    \n",
    "    # for multi-class cls with weighted f1\n",
    "    AVERAGE_MODE = \"weighted\"\n",
    "    f1 = f1_score(actual, predicted, average=AVERAGE_MODE) \n",
    "    print(f\"Test f1: {f1}\")\n",
    "    \n",
    "    def create_iris_confusion_matrix_helper(actual, predicted) -> DataFrame:\n",
    "        \"\"\" create a confustion matrix DataFrame\n",
    "        @param: actual: array-like of shape (n_samples,)\n",
    "        @param: predicted: array-like of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        label_map = {\n",
    "            \"0\": \"Iris-setosa\",\n",
    "            \"1\": \"Iris-versicolor\",\n",
    "            \"2\": \"Iris-virginica\"\n",
    "        }\n",
    "        # the keys are str, need to be converted to int as labels\n",
    "        # for the funciton confusion_matrix\n",
    "        labels = list(map(int, label_map.keys()))\n",
    "        vocab = list(label_map.values())\n",
    "        # vocab = list(np.unique(actual))\n",
    "        cm: np.ndarray = confusion_matrix(actual, predicted, labels = labels)\n",
    "        return pd.DataFrame(cm, index=vocab, columns=vocab)    \n",
    "    \n",
    "    conf_mat_df = create_iris_confusion_matrix_helper(actual, predicted)\n",
    "    print(conf_mat_df)\n",
    "    \n",
    "    # write conf_mat_df to the output path\n",
    "    # with open(cm_output_path, 'w', encoding=\"utf-8\") as f:\n",
    "    #     conf_mat_df.to_csv(f, index=True, encoding=\"utf-8\")\n",
    "    conf_mat_df.to_csv(cm_output_dataset.path, index=True, encoding=\"utf-8\")    \n",
    "        \n",
    "    mod_scores = namedtuple('ModelScores',['model', 'acc', 'f1'])    \n",
    "    return mod_scores(\"tf\", accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f7c6f-39c3-4a75-8d54-0547288ebb30",
   "metadata": {},
   "source": [
    "### Define Confusion Matrix Visualization Component\n",
    "* Visualization with Kubeflow: https://www.kubeflow.org/docs/components/pipelines/v1/sdk/output-viewer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25baa12f-12f6-4a53-867d-2d6fb1826661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(\n",
    "#     create_component_from_func,\n",
    "#     output_component_file=f\"{PREFIX}visualize_confusion_matrix_component.yaml\",\n",
    "#     base_image=settings.base_python_image, # use python base image\n",
    "#     packages_to_install=[\n",
    "#         f\"pandas=={settings.pandas_version}\",\n",
    "#     ]    \n",
    "# )\n",
    "# def confusion_visualization(csv_path: InputPath(), mlpipeline_ui_metadata_path: OutputPath(str)):\n",
    "#     \"\"\"Provide confusion matrix csv file to visualize as metrics.\"\"\"\n",
    "#     import json\n",
    "#     import pandas as pd\n",
    "#     from typing import List, Tuple\n",
    "#     from collections import namedtuple\n",
    "    \n",
    "#     cm_df = pd.read_csv(csv_path, index_col=0)\n",
    "#     print(cm_df)\n",
    "    \n",
    "#     def make_pair_dataframe(df: pd.DataFrame) -> Tuple[pd.DataFrame, List]: \n",
    "#         \"\"\"\n",
    "#         this function constructs a target, predicted, count pair dataframe,\n",
    "#         to be used for kf v1 confusion matrix visualiation\n",
    "#         \"\"\"\n",
    "#         # create a vocabular list from the dataframe column names as List[str]\n",
    "#         vocab = list(map(str, df.columns.values.tolist()))\n",
    "#         data = []\n",
    "#         # use df.to_numpy() to remove the header row for enumerate\n",
    "#         for target_index, target_row in enumerate(cm_df.to_numpy()):\n",
    "#             for predicted_index, count in enumerate(target_row):\n",
    "#                 data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "            \n",
    "#         # convert confusion_matrix pair dataset to dataframe\n",
    "#         df = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "\n",
    "#         # change 'target', 'predicted' to strings\n",
    "#         # df[['target', 'predicted']] = (df[['target', 'predicted']].astype(\"string\")).astype(\"string\")\n",
    "\n",
    "#         vocab = cm_df.columns.values.tolist()\n",
    "#         return df, vocab\n",
    "    \n",
    "#     pair_df, vocab = make_pair_dataframe(cm_df)\n",
    "#     # print(pair_df)\n",
    "#     # print(pair_df.dtypes)\n",
    "#     # print(f\"type of vocab: {type(vocab)}\")\n",
    "#     # print(vocab)\n",
    "    \n",
    "#     '''\n",
    "#     Important: \n",
    "#         make sure the 'label' is a List[str]\n",
    "#         use list(map(str, vocab)) to convert, if not\n",
    "#     '''\n",
    "#     metadata = {\n",
    "#         'outputs' : [{\n",
    "#           'type': 'confusion_matrix',\n",
    "#           'format': 'csv',\n",
    "#           'schema': [\n",
    "#             {'name': 'target', 'type': 'CATEGORY'},\n",
    "#             {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "#             {'name': 'count', 'type': 'NUMBER'},\n",
    "#           ],\n",
    "#           'source': pair_df.to_csv(header=False, index=False),\n",
    "#           'storage': 'inline',\n",
    "#           'labels': vocab,\n",
    "#         }]\n",
    "#     }\n",
    "    \n",
    "#     with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "#         json.dump(metadata, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9b0bb-0938-4371-add7-046a3c9018bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Run Output Component\n",
    "Reference to KF v1 Pipeline Metrics: \n",
    "* https://www.kubeflow.org/docs/components/pipelines/sdk/pipelines-metrics/\n",
    "\n",
    "The `name` and `numberValue` variable shall not be changed.\n",
    "```json\n",
    "            {\n",
    "                \"name\": \"some name\",\n",
    "                \"numberValue\": some value,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"f1 score\",\n",
    "                \"numberValue\": metric_tf,\n",
    "            },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e46208-1558-4eca-9070-b2310571b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @create_component_from_func\n",
    "# def show_performance_metrics(\n",
    "#     model_code: str,\n",
    "#     metric_f1: float,\n",
    "#     metric_acc: float,\n",
    "#     mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "# ):\n",
    "#     import json\n",
    "#     metrics = {\n",
    "#         \"metrics\": [\n",
    "#             {\n",
    "#                 \"name\": f\"{model_code}-acc-score\", \n",
    "#                 \"numberValue\": metric_acc,\n",
    "#                 \"format\": \"PERCENTAGE\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"name\": f\"{model_code}-f1-score\",\n",
    "#                 \"numberValue\": metric_f1,\n",
    "#                 \"format\": \"PERCENTAGE\"\n",
    "#             },\n",
    "#         ],\n",
    "#     } \n",
    "#     with open(mlpipeline_metrics_path, \"w\") as f:\n",
    "#         json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165590ad-f3ba-482f-bcdc-16635f5ab69c",
   "metadata": {},
   "source": [
    "### Define Helper Function\n",
    "Difference between 2Gi and 2G\\\n",
    "https://stackoverflow.com/questions/50804915/kubernetes-size-definitions-whats-the-difference-of-gi-and-g/50805048#50805048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70b748d4-cbb5-421f-b80f-79071990a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_compiled_file_path(file_name: str, prefix=\"\", pipeline_path_dir=\"./compiled\", kfp_version=\"v2_\") -> str:\n",
    "    \"\"\"\n",
    "    In KFP SDK v2, YAML is the preferred serialization format. Json will also work\n",
    "    Reference:\n",
    "    https://www.kubeflow.org/docs/components/pipelines/v2/migration/#sdk-v1-v2-namespace-to-sdk-v2\n",
    "    \"\"\"\n",
    "    return f\"{pipeline_path_dir}/{prefix}{kfp_version}{file_name}.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb43819c-aeb2-4ab0-a395-54dc3c3bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl.pipeline_task import PipelineTask\n",
    "\n",
    "def set_res_limit(task: PipelineTask, mem_req=\"200Mi\", cpu_req=\"2000m\", mem_lim=\"4000Mi\", cpu_lim='4000m') -> PipelineTask:\n",
    "    \"\"\"set the resource limit for cpu and memory, no cpu and memory requirement sofar.\n",
    "    should the limit is set to small, the Task Pod would be stopped by kubernetes with OOMKilled status.\n",
    "    \n",
    "    Args:\n",
    "        task(PipelineTask): the KFP PipelineTask which need to be set the cpu and memory limits\n",
    "        cpu_limit(str): the str representation of cpu limit e.g. '1' as one cpu time, '0.5' as 1/2 cpu time\n",
    "        mem_limit(str): the str representation of memory limit e.g. '500M' for 500MB RAM\n",
    "        \n",
    "    Return:\n",
    "        (PipelineTask): the PipelineTask with the desired limitations set\n",
    "    \"\"\"\n",
    "    # return task.set_cpu_limit('1').set_memory_limit('500M')\n",
    "    return task.set_cpu_request(cpu_req)\\\n",
    "            .set_cpu_limit(cpu_lim)\\\n",
    "            .set_memory_request(mem_req)\\\n",
    "            .set_memory_limit(mem_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30253e28-bf1e-4716-a05b-2524952da9b5",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "* Intro Kubeflow pipeline: https://v1-5-branch.kubeflow.org/docs/components/pipelines/introduction/\n",
    "* Kubeflow pipeline SDK v1: https://v1-5-branch.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f43545d-ddfd-49ad-8f43-fb7600d4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name = EXPERIMENT_NAME,\n",
    "    description = EXPERIMENT_DESC\n",
    ")\n",
    "def custom_pipeline(epochs: int):\n",
    "    '''local variable'''\n",
    "    # no_artifact_cache = False\n",
    "    # artifact_cache_today = True\n",
    "    # cache_setting = no_artifact_cache\n",
    "    label_col_name = \"label\"\n",
    "    feature_col_name = \"features\"\n",
    "    batch_size = 50\n",
    "    # epochs = 100\n",
    "    \n",
    "    '''pipeline'''   \n",
    "    download_task = download_data()\n",
    "    # 200 MB ram and 1 cpu\n",
    "    download_task = set_res_limit(download_task, mem_req=\"500Mi\", cpu_req=\"1000m\")\n",
    "    # set the download caching to be 1day, disable caching with P0D\n",
    "    # download_task.set_caching_options(enable_caching=cache_setting)\n",
    "    # download_task.set_display_name(\"download-iris-data\")\n",
    "    \n",
    "    # variable name \"output_path\", all \"_path\" will be removed by sysem\n",
    "    process_data_task = process_data(\n",
    "        label_col_name=label_col_name, \n",
    "        feature_col_name=feature_col_name,\n",
    "        csv_dataset=download_task.output)\n",
    "    process_data_task = set_res_limit(process_data_task, mem_req=\"500Mi\", cpu_req=\"1000m\")\n",
    "    # BUG, change the display name, the artifacts can not be found.\n",
    "    # process_data_task.set_caching_options(enable_caching=cache_setting)\n",
    "    # process_data_task.set_display_name(\"split-iris-data\")\n",
    "    \n",
    "    # train tensorflow model, input variable are all removed with _path  \n",
    "    train_tf_task = train_tf(\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        label_col_name=label_col_name,\n",
    "        train_csv_dataset=process_data_task.outputs[\"train_csv_dataset\"],\n",
    "        test_csv_dataset=process_data_task.outputs[\"test_csv_dataset\"]\n",
    "    )\n",
    "    train_tf_task = set_res_limit(train_tf_task, mem_req=\"2Gi\", cpu_req=\"2000m\")\n",
    "    # train_tf_task.set_caching_options(enable_caching=cache_setting)\n",
    "    # train_tf_task.set_display_name(\"train-tf-model\")\n",
    "    \n",
    "    # visualize confusion matrix\n",
    "    # visualization_task = confusion_visualization(train_tf_task.outputs[\"cm_output\"])\n",
    "    # visualization_task = pod_resource_transformer(visualization_task, mem_req=\"200Mi\", cpu_req=\"500m\")\n",
    "    # visualization_task.execution_options.caching_strategy.max_cache_staleness = cache_setting\n",
    "    # visualization_task.set_display_name(\"visualize confusion matrix\")\n",
    "    \n",
    "    # show model outputs\n",
    "    # show_ml_metrics_task = show_performance_metrics(\n",
    "    #     model_code = train_tf_task.outputs['model'],\n",
    "    #     metric_f1 = train_tf_task.outputs['f1'],\n",
    "    #     metric_acc =  train_tf_task.outputs['acc'],\n",
    "    # )\n",
    "    # show_ml_metrics_task = pod_resource_transformer(show_ml_metrics_task, mem_req=\"200Mi\", cpu_req=\"500m\")\n",
    "    # show_ml_metrics_task.execution_options.caching_strategy.max_cache_staleness =cache_setting\n",
    "    # show_ml_metrics_task.set_display_name(\"output model metrics\")\n",
    "\n",
    "my_pipeline = custom_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e789aa-5ccc-4290-8d9d-b3297128417b",
   "metadata": {},
   "source": [
    "### (optional) pipeline compile step\n",
    "use the following command to compile the pipeline to IR YAML serialized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb19bcb-2215-4ca6-93a8-bd65a31f6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "import os\n",
    "\n",
    "component_list = []\n",
    "\n",
    "# download_data\n",
    "component_list.append((\n",
    "    download_data, \n",
    "    gen_compiled_file_path(file_name=\"comp_download_iris_data\", prefix=PREFIX)\n",
    ")) \n",
    "# process_data\n",
    "component_list.append((\n",
    "    process_data, \n",
    "    gen_compiled_file_path(file_name=\"comp_process_iris_data\", prefix=PREFIX)\n",
    "))\n",
    "# train_tf\n",
    "component_list.append((\n",
    "    process_data, \n",
    "    gen_compiled_file_path(file_name=\"comp_train_tf_iris_data\", prefix=PREFIX)\n",
    "))\n",
    "\n",
    "my_pipeline_file_name = \"pipeline_iris_demo\"\n",
    "pipeline_package_path = gen_compiled_file_path(my_pipeline_file_name, prefix=PREFIX)\n",
    "\n",
    "pipeline_path_dir=\"./compiled\"\n",
    "if not os.path.exists(pipeline_path_dir):\n",
    "    os.makedirs(pipeline_path_dir)\n",
    "\n",
    "# compile component, instead of using output_component_file in the @dsl.component decorator\n",
    "for comp in component_list:\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func=comp[0],\n",
    "        package_path=comp[1],\n",
    "    )\n",
    "\n",
    "# compile pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198c16-f9ab-4e4c-a2f2-d710bec7aba9",
   "metadata": {},
   "source": [
    "### Create Experiment Run\n",
    "\n",
    "create run label with current data time\n",
    "```python\n",
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "ts = datetime.strftime(datetime.now(ptimezone(\"Europe/Berlin\")), \"%Y-%m-%d %H-%M-%S\")\n",
    "print(ts)\n",
    "```\n",
    "\n",
    "Reference:\n",
    "* https://stackoverflow.com/questions/25837452/python-get-current-time-in-right-timezone/25887393#25887393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08dd9330-1a3f-4fcd-8e53-fa7e4a05de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone as ptimezone\n",
    "\n",
    "def get_local_time_str(target_tz_str: str = \"Europe/Berlin\", format_str: str = \"%Y-%m-%d %H-%M-%S\") -> str:\n",
    "    \"\"\"\n",
    "    this method is created since the local timezone is miss configured on the server\n",
    "    @param: target timezone str default \"Europe/Berlin\"\n",
    "    @param: \"%Y-%m-%d %H-%M-%S\" returns 2022-07-07 12-08-45\n",
    "    \"\"\"\n",
    "    target_tz = ptimezone(target_tz_str) # create timezone, in python3.9 use standard lib ZoneInfo\n",
    "    # utc_dt = datetime.now(datetime.timezone.utc)\n",
    "    target_dt = datetime.now(target_tz)\n",
    "    return datetime.strftime(target_dt, format_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1ed73-9fd8-4e59-855e-690e738d09d9",
   "metadata": {},
   "source": [
    "### Config pipeline run\n",
    "* Setting imagePullSecretes for Pipeline with SDK: https://github.com/kubeflow/pipelines/issues/5843#issuecomment-859799181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853837b3-604d-439b-9cea-4b8e226c0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kubernetes import client as k8s_client\n",
    "# pipeline_config = dsl.PipelineConf()\n",
    "\n",
    "# pipeline_config.set_image_pull_secrets([k8s_client.V1ObjectReference(name=K8_GIT_SECRET_NAME, namespace=NAME_SPACE)])\n",
    "# pipeline_config.set_image_pull_policy(\"Always\")\n",
    "# pipeline_config.set_image_pull_policy(\"IfNotPresent\")\n",
    "\n",
    "pipeline_args = {\n",
    "    \"epochs\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5264dc44-9abd-48a2-970e-3ed8693edfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/20ad43d6-4e20-494c-9a81-52869a440fd3\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/20aba077-3a13-46af-8de8-1c4a3f3b84e3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=20aba077-3a13-46af-8de8-1c4a3f3b84e3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUN_NAME = f\"{PREFIX}v2_iris_demo {get_local_time_str()}\"\n",
    "# RUN_NAME = RUN_NAME.replace(\"_\",\"-\")\n",
    "\n",
    "# CACHING_SETTING = False\n",
    "CACHING_SETTING = True\n",
    "\n",
    "# client = kfp.Client()\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    pipeline_func=custom_pipeline,\n",
    "    arguments = pipeline_args, #{}\n",
    "    run_name = RUN_NAME,\n",
    "    # pipeline_conf=pipeline_config,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    namespace=NAMESPACE,\n",
    "    enable_caching=CACHING_SETTING,\n",
    ")\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd6d15-c88d-4d5a-9b4e-fb2f3706c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
